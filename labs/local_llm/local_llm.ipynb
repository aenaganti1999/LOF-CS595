{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell once to set up your conversation. \n",
    "\n",
    "# Set up your assistant prompt and initialize the message thread.\n",
    "\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"Answer the user's question from the perspective of THE world's leading applicable domain knowledge expert. Process: 1. Rephrase the question \n",
    "to anticipate the user's needs. 2. Break the question into subquestions. 2. Assemble answers in a step-by-step manner organized for optimal comprehension. 3. Include Google Scholar and Google links\n",
    "formatted as follows, that is, do not include direct links to articles, instead search for related terms as follows.:\n",
    "\n",
    "- _See also:_ [Related topics for deeper understanding]\n",
    "  üìö[Research topic articles](https://scholar.google.com/scholar?q=related+terms)\n",
    "  üîç[General information](https://www.google.com/search?q=related+terms)\n",
    "\n",
    "- _You may also enjoy:_ [Topics of tangential interest]\n",
    "  üåü[Explore more](https://www.google.com/search?q=tangential+interest+terms)\n",
    "\n",
    "\"\"\"\n",
    "message_thread = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# add your first and subsequent questions here! Rerun this cell and the next. \n",
    "\n",
    "user_prompt = \"How can I avoid overeating?\"\n",
    "\n",
    "\n",
    "# Define the output file path for the conversation\n",
    "output_file = \"/Users/anvithaenaganti/Documents/GitHub/LOF-CS595/labs/local_llm/output.md\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from /Users/anvithaenaganti/Documents/GitHub/LOF-CS595/labs/local_llm/Meta-Llama-3-8B-Instruct-Q4_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"ƒ† ƒ†\", \"ƒ† ƒ†ƒ†ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  22:                      quantize.imatrix.file str              = /models/Meta-Llama-3-8B-Instruct-GGUF...\n",
      "llama_model_loader: - kv  23:                   quantize.imatrix.dataset str              = /training_data/groups_merged.txt\n",
      "llama_model_loader: - kv  24:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  25:              quantize.imatrix.chunks_count i32              = 88\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  217 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.36 GiB (4.67 BPW) \n",
      "llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 '√Ñ'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  4096.00 MiB, ( 4096.06 /  5461.34)\n",
      "\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =   501.00 MiB, ( 4597.06 /  5461.34)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   281.81 MiB\n",
      "llm_load_tensors:      Metal buffer size =  4186.00 MiB\n",
      ".......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1\n",
      "ggml_metal_init: picking default device: Apple M1\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  =  5726.63 MB\n",
      "llama_kv_cache_init:      Metal KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      Metal compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     9.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.entries_count': '224', 'general.quantization_version': '2', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128001', 'quantize.imatrix.file': '/models/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct.imatrix', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.pre': 'llama-bpe', 'quantize.imatrix.chunks_count': '88', 'tokenizer.ggml.model': 'gpt2', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'llama.context_length': '8192', 'llama.attention.head_count': '32', 'general.file_type': '14', 'llama.feed_forward_length': '14336', 'quantize.imatrix.dataset': '/training_data/groups_merged.txt', 'llama.rope.dimension_count': '128', 'llama.rope.freq_base': '500000.000000', 'llama.embedding_length': '4096', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'Meta-Llama-3-8B-Instruct', 'llama.block_count': '32'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "\n",
      "llama_print_timings:        load time =    9246.77 ms\n",
      "llama_print_timings:      sample time =     257.45 ms /   300 runs   (    0.86 ms per token,  1165.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9246.44 ms /   212 tokens (   43.62 ms per token,    22.93 tokens per second)\n",
      "llama_print_timings:        eval time =   34360.85 ms /   299 runs   (  114.92 ms per token,     8.70 tokens per second)\n",
      "llama_print_timings:       total time =   52824.16 ms /   511 tokens\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# The following function dynamically updates the output file as responses are generated.\n",
    "\n",
    "def clear_output_file():\n",
    "    \"\"\"\n",
    "    Clears all content from the output file by writing an empty string\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\") as file:\n",
    "        file.write(\"\")  # Clear the file by writing an empty string\n",
    "\n",
    "\n",
    "def write_to_file(content):\n",
    "    \"\"\"\n",
    "    Appends new content to the output file\n",
    "    Args:\n",
    "        content: Text content to append to the file\n",
    "    \"\"\"\n",
    "    with open(output_file, \"a\") as file:\n",
    "        file.write(content)\n",
    "    # If the file is not already open in VSCode, uncomment the next line\n",
    "    #os.system(f\"code {output_file}\")\n",
    "    # Best, though, to have the file open and in preview mode. Then you see output\n",
    "    # as it is generated!\n",
    "    \n",
    "    # Add the latest prompt to the thread\n",
    "message_thread.append({\"role\": \"user\", \"content\": user_prompt}) \n",
    "\n",
    "\n",
    "# Append the Markdown content to a file\n",
    "with open(output_file, \"a\") as file:\n",
    "    file.write(f'\\n\\n _____ \\n\\n User üë©‚Äç‚öïÔ∏è: {user_prompt} \\n\\n ')\n",
    "\n",
    "llm = Llama(\n",
    "      model_path =\"/Users/anvithaenaganti/Documents/GitHub/LOF-CS595/labs/local_llm/Meta-Llama-3-8B-Instruct-Q4_K_S.gguf\",\n",
    "      chat_format = \"llama-3\",\n",
    "      verbose=True,\n",
    "      n_gpu_layers=100, # Uncomment to use GPU acceleration\n",
    "      #seed=1337, # Uncomment to set a specific seed\n",
    "      n_ctx=512, # Uncomment to increase the context window\n",
    "  )\n",
    "output = llm.create_chat_completion(\n",
    "        messages = message_thread,\n",
    "        max_tokens=1000,\n",
    "        stream=True,\n",
    "        stop = \"<|assistant|>\",\n",
    "        )\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "for chunk in output:\n",
    "    delta = chunk['choices'][0]['delta']\n",
    "    if 'role' in delta:\n",
    "        role_text = delta['role'] + \"üëΩ\" + \": \"\n",
    "        # print(role_text, end='')\n",
    "        text += role_text\n",
    "        write_to_file(role_text)\n",
    "    elif 'content' in delta:\n",
    "        content_text = delta['content']\n",
    "        # print(content_text, end='')\n",
    "        text += content_text\n",
    "        write_to_file(content_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If the context window is big enough, add the latest response to the thread. You can then go back to update the user prompt and send again.\n",
    "# message_thread.append({\"role\": \"system\", \"content\": text})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, if you want to clear your output!\n",
    "\n",
    "\n",
    "clear_output_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Students will need to fill in the critical functions\n",
    "# Hints are provided within the comments.\n",
    "# -------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operating system interface - for file and path operations\n",
    "import os\n",
    "\n",
    "# NumPy - Scientific computing library for array operations and numerical computations\n",
    "import numpy as np\n",
    "\n",
    "# PDF processing library - for reading and extracting text from PDF files\n",
    "import PyPDF2\n",
    "\n",
    "# Sentence Transformers - for creating semantic embeddings from text\n",
    "# (Note: This import appears unused in the current code as you're using Llama for embeddings)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "# Facebook AI Similarity Search (FAISS) - for efficient similarity search and clustering of dense vectors\n",
    "# Used here for storing and querying text embeddings\n",
    "import faiss\n",
    "\n",
    "# Python bindings for the llama.cpp library - provides access to Llama language models\n",
    "# Used for text generation and creating embeddings\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinical\n",
      "Profile:\n",
      "Patient\n",
      "Demographics:\n",
      "‚óè\n",
      "Age:\n",
      "57\n",
      "years\n",
      "‚óè\n",
      "Gender:\n",
      "Male\n",
      "‚óè\n",
      "Ethnicity:\n",
      "Non-Hispanic\n",
      "‚óè\n",
      "BMI:\n",
      "31\n",
      "kg/m¬≤\n",
      "(Obese)\n",
      "Medical\n",
      "History:\n",
      "‚óè\n",
      "Primary\n",
      "Condition:\n",
      "Type\n",
      "2\n",
      "Diabetes\n",
      "Mellitus\n",
      "(T2DM)\n",
      "(Diagnosed\n",
      "12\n",
      "years\n",
      "ago)\n",
      "‚óè\n",
      "Glycemic\n",
      "Control:\n",
      "‚óã\n",
      "Last\n",
      "HbA1c:\n",
      "8.3%\n",
      "(Above\n",
      "target)\n",
      "‚óã\n",
      "Fasting\n",
      "Glucose:\n",
      "160\n",
      "mg/dL\n",
      "‚óè\n",
      "Medications:\n",
      "‚óã\n",
      "Metformin\n",
      "1000\n",
      "mg\n",
      "twice\n",
      "daily\n",
      "‚óã\n",
      "Insulin\n",
      "Glargine\n",
      "20\n",
      "units\n",
      "at\n",
      "bedtime\n",
      "‚óã\n",
      "Lisinopril\n",
      "10\n",
      "mg\n",
      "once\n",
      "daily\n",
      "for\n",
      "hypertension\n",
      "‚óã\n",
      "Atorvastatin\n",
      "40\n",
      "mg\n",
      "once\n",
      "daily\n",
      "for\n",
      "hyperlipidemia\n",
      "‚óè\n",
      "Non-adherence\n",
      "Issues:\n",
      "Occasionally\n",
      "misses\n",
      "insulin\n",
      "doses\n",
      "(2‚Äì3\n",
      "days\n",
      "per\n",
      "week),\n",
      "poor\n",
      "diet\n",
      "management\n",
      "‚óè\n",
      "Comorbidities:\n",
      "‚óã\n",
      "Hypertension:\n",
      "Controlled\n",
      "with\n",
      "medication\n",
      "‚óã\n",
      "Dyslipidemia:\n",
      "Partially\n",
      "controlled\n",
      "(LDL\n",
      "110\n",
      "mg/dL)\n",
      "‚óã\n",
      "Obesity:\n",
      "BMI\n",
      "of\n",
      "31\n",
      "kg/m¬≤,\n",
      "limited\n",
      "physical\n",
      "activity\n",
      "‚óã\n",
      "Diabetic\n",
      "Retinopathy:\n",
      "Mild\n",
      "background\n",
      "retinopathy\n",
      "(diagnosed\n",
      "2\n",
      "years\n",
      "ago)\n",
      "‚óã\n",
      "Diabetic\n",
      "Neuropathy:\n",
      "Tingling\n",
      "and\n",
      "numbness\n",
      "in\n",
      "feet\n",
      "‚óã\n",
      "Chronic\n",
      "Kidney\n",
      "Disease:\n",
      "Stage\n",
      "2\n",
      "(GFR\n",
      "65\n",
      "mL/min/1.73\n",
      "m¬≤)\n",
      "Social\n",
      "History:\n",
      "‚óè\n",
      "Smoking:\n",
      "Non-smoker\n",
      "‚óè\n",
      "Alcohol:\n",
      "Drinks\n",
      "occasionally\n",
      "‚óè\n",
      "Physical\n",
      "Activity:\n",
      "Sedentary,\n",
      "walks\n",
      "for\n",
      "10\n",
      "minutes\n",
      "a\n",
      "day\n",
      "Lifestyle\n",
      "Factors:\n",
      "‚óè\n",
      "Diet:\n",
      "Consumes\n",
      "a\n",
      "diet\n",
      "high\n",
      "in\n",
      "carbohydrates,\n",
      "irregular\n",
      "meal\n",
      "patterns\n",
      "‚óè\n",
      "Exercise:\n",
      "Very\n",
      "low\n",
      "activity\n",
      "level,\n",
      "finds\n",
      "it\n",
      "hard\n",
      "to\n",
      "incorporate\n",
      "physical\n",
      "activity\n",
      "into\n",
      "daily\n",
      "life\n",
      "Patient\n",
      "Health\n",
      "Assessment\n",
      "Questionnaire:\n",
      "1.\n",
      "Blood\n",
      "Sugar\n",
      "Monitoring:\n",
      "‚óã\n",
      "How\n",
      "often\n",
      "do\n",
      "you\n",
      "check\n",
      "your\n",
      "blood\n",
      "sugar\n",
      "levels\n",
      "at\n",
      "home?\n",
      "2.\n",
      "Physical\n",
      "Activity:\n",
      "‚óã\n",
      "How\n",
      "many\n",
      "days\n",
      "per\n",
      "week\n",
      "do\n",
      "you\n",
      "engage\n",
      "in\n",
      "physical\n",
      "activity,\n",
      "and\n",
      "what\n",
      "kind\n",
      "of\n",
      "activities\n",
      "do\n",
      "you\n",
      "do?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the PDF file\n",
    "pdf_file = \"Sample Patient 1.pdf\" \n",
    "\n",
    "# Create a PDF reader object to read the file\n",
    "pdf_reader = PyPDF2.PdfReader(open(pdf_file, \"rb\"))\n",
    "pdf_text = \"\"\n",
    "\n",
    "# Iterate through each page of the PDF\n",
    "for page_num in range(len(pdf_reader.pages)):\n",
    "    # Extract text from the current page\n",
    "    page_text = pdf_reader.pages[page_num].extract_text()\n",
    "    # If text was successfully extracted, add it to our accumulated text\n",
    "    if page_text:\n",
    "        pdf_text += page_text + \"\\n\"\n",
    "print(pdf_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 1\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size=500, chunk_overlap=50) -> list:\n",
    "    \"\"\"\n",
    "    Splits text into overlapping chunks for processing\n",
    "    Args:\n",
    "        text: Input text to be chunked\n",
    "        chunk_size: Number of words per chunk\n",
    "        chunk_overlap: Number of overlapping words between chunks\n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    HINTS:\n",
    "      - Split the input text into a list of words.\n",
    "      - Use a while loop to keep creating chunks until you've processed all words.\n",
    "      - For each chunk:\n",
    "        1. Calculate the end position based on `chunk_size`.\n",
    "        2. Join the words from `start` to `end` into a single string.\n",
    "        3. Append this chunk to a list.\n",
    "        4. Adjust `start` by subtracting `chunk_overlap` to ensure overlapping words.\n",
    "      - Make sure `start` never goes below zero.\n",
    "      - Return the list of chunks.\n",
    "    \n",
    "    #TODO: Implement the function\n",
    "    pass\n",
    "\n",
    "# Process the PDF text into chunks and print the total number of chunks created\n",
    "text_chunks = chunk_text(pdf_text, chunk_size=500, chunk_overlap=50)\n",
    "print(f\"Number of text chunks: {len(text_chunks)}\")\n",
    "`\n",
    "    def chunk_text(text, chunk_size=500, chunk_overlap=50) -> list:\n",
    "    \n",
    "    Splits text into overlapping chunks for processing.\n",
    "    Args:\n",
    "        text: Input text to be chunked.\n",
    "        chunk_size: Number of words per chunk.\n",
    "        chunk_overlap: Number of overlapping words between chunks.\n",
    "    Returns:\n",
    "        List of text chunks.\n",
    "    \"\"\"\n",
    "    words = text.split()  # Split text into words\n",
    "    chunks = []  # List to store the chunks\n",
    "    start = 0  # Initialize start index\n",
    "    \n",
    "    while start < len(words):\n",
    "        end = min(start + chunk_size, len(words))  # Ensure end doesn't exceed length\n",
    "        chunk = \" \".join(words[start:end])  # Join words to form a chunk\n",
    "        chunks.append(chunk)  # Append chunk to the list\n",
    "        \n",
    "        # Move start index forward while maintaining overlap\n",
    "        start += chunk_size - chunk_overlap  \n",
    "        if start < 0:\n",
    "            start = 0  # Ensure start is never below zero\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Process the PDF text into chunks and print the total number of chunks created  # Replace with actual text\n",
    "text_chunks = chunk_text(pdf_text, chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "print(f\"Number of text chunks: {len(text_chunks)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama model loaded with embedding support.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/Users/anvithaenaganti/Documents/GitHub/LOF-CS595/labs/local_llm/Meta-Llama-3-8B-Instruct-Q4_K_S.gguf\"\n",
    "\n",
    "llm_for_embeddings = Llama(\n",
    "    model_path=model_path,    # Path to the Llama model file\n",
    "    n_ctx=2048,              # Context window size\n",
    "    embedding=True,          # Enable embedding generation\n",
    "    verbose=False            # Disable verbose output\n",
    ")\n",
    "\n",
    "print(\"Llama model loaded with embedding support.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances: [[0.14693439]]\n",
      "Indices: [[0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "def llama_embed_text(text) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates embeddings for input text using Llama model.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to embed.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A 4096-dimensional mean embedding vector.\n",
    "    \"\"\"\n",
    "    embedding_result = llm_for_embeddings.create_embedding(text)\n",
    "\n",
    "    if isinstance(embedding_result, dict) and 'data' in embedding_result:\n",
    "        token_embeddings = embedding_result['data'][0]['embedding']\n",
    "    else:\n",
    "        raise ValueError(\"The embedding result does not contain the expected 'data' field.\")\n",
    "\n",
    "    token_embeddings_np = np.array(token_embeddings, dtype=np.float32)\n",
    "\n",
    "    mean_embedding = np.mean(token_embeddings_np, axis=0)\n",
    "\n",
    "    return mean_embedding\n",
    "\n",
    "\n",
    "# Process embeddings for all text chunks\n",
    "\n",
    "\n",
    "all_embeddings = [llama_embed_text(chunk) for chunk in text_chunks]\n",
    "\n",
    "# Stack embeddings into (N, 4096) matrix\n",
    "embeddings_matrix = np.vstack(all_embeddings).astype(\"float32\")\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "embeddings_matrix /= np.linalg.norm(embeddings_matrix, axis=1, keepdims=True)\n",
    "\n",
    "# Initialize FAISS index for cosine similarity\n",
    "index = faiss.IndexFlatIP(4096)\n",
    "index.add(embeddings_matrix)\n",
    "\n",
    "# Convert query to an embedding and normalize\n",
    "query_emb = llama_embed_text(\"Query text to compare.\")\n",
    "query_emb /= np.linalg.norm(query_emb)\n",
    "\n",
    "# Adjust k to the dataset size to prevent errors\n",
    "k = min(3, len(text_chunks))  # Only search for available matches\n",
    "D, I = index.search(np.array([query_emb], dtype=\"float32\"), k=k)\n",
    "\n",
    "# Print distances and indices\n",
    "print(\"Distances:\", D)\n",
    "print(\"Indices:\", I)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new FAISS index using L2 (Euclidean) distance metric\n",
    "# embeddings_matrix.shape[1] specifies the dimensionality of our vectors (4096 in this case)\n",
    "index = faiss.IndexFlatL2(embeddings_matrix.shape[1])\n",
    "\n",
    "# Add all our document embeddings to the index\n",
    "index.add(embeddings_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef search_similar_chunks(query, k=3) -> list:\\n   \\n    \\n    Searches for text chunks similar to the query using FAISS\\n    Args:\\n        query: Search query text\\n        k: Number of results to return\\n    Returns:\\n        List of tuples containing (text_chunk, distance)\\n    HINTS:\\n    \\n      Convert \\'query\\' to an embedding.\\n      query_emb = llama_embed_text(query)  # Convert query text to embedding\\n\\n      2) Call index.search(...) to find the top k matches.\\n      distances, indices = index.search(query_emb, k)\\n\\n      3) Return a list of (text_chunk, distance) pairs.\\n      \\n      4) Make sure \\'query_emb\\' is reshaped to (1, -1) before searching.\\n      5) Use the indices from index.search to retrieve the chunk texts.\\n    \\n    #TODO: Implement the function\\n    pass\\n\\nExample usage of the search function\\ntest_query = \"How can I avoid overeating?\"\\nsearch_results = search_similar_chunks(test_query, k=3)\\n\\nPrint each result with its similarity score\\nfor i, (chunk, dist) in enumerate(search_results):\\n    print(f\"--- Result {i+1} (distance {dist:.4f}) ---\\n{chunk}\\n\")\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Result 1 (distance 5547.1211) ---\n",
      "Clinical Profile: Patient Demographics: ‚óè Age: 57 years ‚óè Gender: Male ‚óè Ethnicity: Non-Hispanic ‚óè BMI: 31 kg/m¬≤ (Obese) Medical History: ‚óè Primary Condition: Type 2 Diabetes Mellitus (T2DM) (Diagnosed 12 years ago) ‚óè Glycemic Control: ‚óã Last HbA1c: 8.3% (Above target) ‚óã Fasting Glucose: 160 mg/dL ‚óè Medications: ‚óã Metformin 1000 mg twice daily ‚óã Insulin Glargine 20 units at bedtime ‚óã Lisinopril 10 mg once daily for hypertension ‚óã Atorvastatin 40 mg once daily for hyperlipidemia ‚óè Non-adherence Issues: Occasionally misses insulin doses (2‚Äì3 days per week), poor diet management ‚óè Comorbidities: ‚óã Hypertension: Controlled with medication ‚óã Dyslipidemia: Partially controlled (LDL 110 mg/dL) ‚óã Obesity: BMI of 31 kg/m¬≤, limited physical activity ‚óã Diabetic Retinopathy: Mild background retinopathy (diagnosed 2 years ago) ‚óã Diabetic Neuropathy: Tingling and numbness in feet ‚óã Chronic Kidney Disease: Stage 2 (GFR 65 mL/min/1.73 m¬≤) Social History: ‚óè Smoking: Non-smoker ‚óè Alcohol: Drinks occasionally ‚óè Physical Activity: Sedentary, walks for 10 minutes a day Lifestyle Factors: ‚óè Diet: Consumes a diet high in carbohydrates, irregular meal patterns ‚óè Exercise: Very low activity level, finds it hard to incorporate physical activity into daily life Patient Health Assessment Questionnaire: 1. Blood Sugar Monitoring: ‚óã How often do you check your blood sugar levels at home? 2. Physical Activity: ‚óã How many days per week do you engage in physical activity, and what kind of activities do you do?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def search_similar_chunks(query, k=3) -> list:\n",
    "    \"\"\"\n",
    "    Searches for text chunks similar to the query using FAISS.\n",
    "\n",
    "    Args:\n",
    "        query (str): Search query text.\n",
    "        k (int): Number of results to return.\n",
    "\n",
    "    Returns:\n",
    "        List of tuples containing (text_chunk, distance).\n",
    "    \"\"\"\n",
    "    # 1) Convert query to an embedding\n",
    "    query_emb = llama_embed_text(query)\n",
    "\n",
    "    # 4) Make sure 'query_emb' is reshaped to (1, -1) before searching\n",
    "    query_emb = np.array([query_emb], dtype=\"float32\")  # Shape (1, 4096)\n",
    "\n",
    "    # 2) Call index.search(...) to find the top k matches\n",
    "    distances, indices = index.search(query_emb, k)\n",
    "\n",
    "    #  5) Use the indices from index.search to retrieve the chunk texts\n",
    "    results = []\n",
    "    for i in range(k):\n",
    "        idx = indices[0][i]  # Get the index of the matching chunk\n",
    "        if idx == -1:  # FAISS returns -1 when there aren't enough results\n",
    "            continue\n",
    "        results.append((text_chunks[idx], distances[0][i]))  # Store (text_chunk, distance)\n",
    "\n",
    "    #3) Return a list of (text_chunk, distance) pairs\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage of the search function\n",
    "test_query = \"How can I avoid overeating?\"\n",
    "search_results = search_similar_chunks(test_query, k=3)\n",
    "\n",
    "# Print each result with its similarity score\n",
    "for i, (chunk, dist) in enumerate(search_results):\n",
    "    print(f\"--- Result {i+1} (distance {dist:.4f}) ---\\n{chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Initialize the Llama model for chat interactions\\n\\nllm_for_chat = Llama(\\n    model_path=model_path,    # Path to the Llama model weights file\\n    n_ctx=2048,              # Maximum context window size (in tokens)\\n    verbose=True             # Enable detailed logging output\\n)\\n\\ndef run_llm(prompt: str) -> str:\\n    \\n    Runs the Llama model with a simple prompt\\n    Args:\\n        prompt: Input prompt text\\n    Returns:\\n        Generated response from the model\\n    HINTS: \\n      1) Create the \\'messages\\' list with system and user roles.\\n      2) Call llm_for_chat.create_chat_completion() with those messages.\\n      3) Return the text from response[\"choices\"][0][\"message\"][\"content\"].\\n   \\n    #TODO: Implement the function\\n    pass\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from /Users/anvithaenaganti/Documents/GitHub/LOF-CS595/labs/local_llm/Meta-Llama-3-8B-Instruct-Q4_K_S.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"ƒ† ƒ†\", \"ƒ† ƒ†ƒ†ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  22:                      quantize.imatrix.file str              = /models/Meta-Llama-3-8B-Instruct-GGUF...\n",
      "llama_model_loader: - kv  23:                   quantize.imatrix.dataset str              = /training_data/groups_merged.txt\n",
      "llama_model_loader: - kv  24:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  25:              quantize.imatrix.chunks_count i32              = 88\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  217 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.36 GiB (4.67 BPW) \n",
      "llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 '√Ñ'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4467.80 MiB\n",
      ".......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.entries_count': '224', 'general.quantization_version': '2', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128001', 'quantize.imatrix.file': '/models/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct.imatrix', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.ggml.pre': 'llama-bpe', 'quantize.imatrix.chunks_count': '88', 'tokenizer.ggml.model': 'gpt2', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'llama.context_length': '8192', 'llama.attention.head_count': '32', 'general.file_type': '14', 'llama.feed_forward_length': '14336', 'quantize.imatrix.dataset': '/training_data/groups_merged.txt', 'llama.rope.dimension_count': '128', 'llama.rope.freq_base': '500000.000000', 'llama.embedding_length': '4096', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'Meta-Llama-3-8B-Instruct', 'llama.block_count': '32'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama  # Import the Llama model class\n",
    "\n",
    "#  Initialize the Llama model for chat interactions\n",
    "llm_for_chat = Llama(\n",
    "    model_path=\"/Users/anvithaenaganti/Documents/GitHub/LOF-CS595/labs/local_llm/Meta-Llama-3-8B-Instruct-Q4_K_S.gguf\",  # Path to the Llama model weights file\n",
    "    n_ctx=2048,             # Maximum context window size (in tokens)\n",
    "    verbose=True            # Enable detailed logging output\n",
    ")\n",
    "\n",
    "def run_llm(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Runs the Llama model with a simple prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): Input prompt text.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated response from the model.\n",
    "    \"\"\"\n",
    "    #  1) Create the 'messages' list with system and user roles\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    # 2) Call llm_for_chat.create_chat_completion() with those messages\n",
    "    response = llm_for_chat.create_chat_completion(messages=messages)\n",
    "\n",
    "    #  3) Return the text from response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm_with_pdf_knowledge(user_query, k=1):\n",
    "    \"\"\"\n",
    "    Runs the Llama model with context from similar PDF chunks to provide informed answers.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's question to be answered.\n",
    "        k (int): Number of similar chunks to retrieve from the PDF (default: 3).\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated response incorporating knowledge from the PDF context.\n",
    "    \n",
    "    Steps:\n",
    "      1) Retrieve top-k similar chunks using search_similar_chunks.\n",
    "      2) Combine them into a single 'context' string.\n",
    "      3) Construct a prompt including the PDF context and the user query.\n",
    "      4) Pass that prompt to run_llm(prompt).\n",
    "      5) Return the final response text.\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve top-k similar chunks from the PDF\n",
    "    similar_chunks = search_similar_chunks(user_query, k)\n",
    "\n",
    "    # Step 2: Combine the retrieved chunks into a single context string\n",
    "    context = \"\\n\\n\".join(similar_chunks)\n",
    "\n",
    "    # Step 3: Construct a prompt that includes both the context and the user query\n",
    "    prompt = f\"\"\"You are an AI assistant with knowledge extracted from a PDF document. \n",
    "Below is relevant context from the document:\n",
    "\n",
    "{context}\n",
    "\n",
    "Based on this information, answer the following question:\n",
    "\n",
    "{user_query}\n",
    "\"\"\"\n",
    "\n",
    "    # Step 4: Pass the constructed prompt to the Llama model\n",
    "    response = run_llm(prompt)\n",
    "\n",
    "    # Step 5: Return the generated response\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   22393.90 ms\n",
      "llama_print_timings:      sample time =      37.68 ms /   105 runs   (    0.36 ms per token,  2786.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22393.32 ms /   459 tokens (   48.79 ms per token,    20.50 tokens per second)\n",
      "llama_print_timings:        eval time =  739009.63 ms /   104 runs   ( 7105.86 ms per token,     0.14 tokens per second)\n",
      "llama_print_timings:       total time =  762492.53 ms /   563 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Whats is the avg cgm values for my patient?\n",
      "A: I'm happy to help! However, I must point out that there is no mention of Continuous Glucose Monitoring (CGM) values in the provided context. The document only mentions HbA1c and fasting glucose levels.\n",
      "\n",
      "HbA1c: 8.3%\n",
      "Fasting Glucose: 160 mg/dL\n",
      "\n",
      "There is no information available to estimate average CGM values for this patient. If you would like, I can help with any other questions or provide insights based on the provided information!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   22393.90 ms\n",
      "llama_print_timings:      sample time =      56.87 ms /   153 runs   (    0.37 ms per token,  2690.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6794.78 ms /    16 tokens (  424.67 ms per token,     2.35 tokens per second)\n",
      "llama_print_timings:        eval time = 1068627.66 ms /   152 runs   ( 7030.45 ms per token,     0.14 tokens per second)\n",
      "llama_print_timings:       total time = 1077098.79 ms /   168 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Whats is the avg cgm values for my patient?\n",
      "A: I'd be happy to help! However, I need more information from you. Could you please provide me with some details about your patient's CGM (Continuous Glucose Monitoring) data?\n",
      "\n",
      "Here are some questions to help me better understand their situation:\n",
      "\n",
      "1. What is the patient's age?\n",
      "2. What is their diagnosis (e.g., type 1 diabetes, type 2 diabetes, gestational diabetes)?\n",
      "3. Are they taking any medications or insulin therapy?\n",
      "4. What is their typical daily activity level (sedentary, moderately active, highly active)?\n",
      "5. Have they been experiencing any symptoms or issues with their glucose levels recently?\n",
      "\n",
      "Once I have this information, I can provide you with a more accurate average CGM value for your patient.\n"
     ]
    }
   ],
   "source": [
    "new_query = \"Whats is the avg cgm values for my patient?\"\n",
    "# First call: Uses PDF context\n",
    "new_answer = run_llm_with_pdf_knowledge(new_query)\n",
    "print(\"Q:\", new_query)\n",
    "print(\"A:\", new_answer)\n",
    "# Second call: Direct LLM query without context\n",
    "new_answer = run_llm(new_query)\n",
    "print(\"Q:\", new_query)\n",
    "print(\"A:\", new_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
